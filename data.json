[
    {
        "title": "The Phantom Plateau",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "1-13",
        "authors": ["Keller, F. S."],
        "url": "https://onlinelibrary.wiley.com/doi/10.1901/jeab.1958.1-1",
        "process": "Taking Dictation",
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "None"
    },
    {
        "title": "A Conjunctive Schedule of Reinforcement",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "15-24",
        "authors": ["Herrnstein, R. J.", "Morse, W. H."],
        "url": "https://doi.org/10.1901/jeab.1958.1-15",
        "process": ["Conjunctive Schedules", "Schedule: Fixed Interval", "Schedule: Fixed Ratio", "Reinforcement: Positive"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "", 
        "abstract": "Two pigeons were trained on a 15-minute, fixed-interval schedule. They were then trained on the conjunctive fixed-interval, fixed-ratio schedule, in which a response is reinforced only after the passage of a specified time and the emission of a minimal number of unreinforced responses. The period of time was kept at 15 minutes, while the number requirement was varied from 10 to 240 responses. Increasing the number requirement had the effect of decreasing the average rate of responding. The presence of the number requirement also changed the patternof responding within the 15-minute period from that obtained with ordinary fixed-interval reinforcement."
    }, 
    {
        "title": "Operant Extinction After Fixed-Interval Schedules with Young Children",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "25-29",
        "authors": ["Bijou, S. W."],
        "url": "https://doi.org/10.1901/jeab.1958.1-25",
        "process": ["Operant Extinction", "Schedule: Fixed Interval", "Reinforcement: Positive"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "Four preschool children (~4 years old) were trained on fixed-interval schedules (FI 20 s, FI 30 s, FI 60 s) using plastic trinkets and, at times, dispenser-motor hum as reinforcers. After several conditioning sessions (3–4 per child, spaced ~22–29 days), extinction was introduced following a brief sequence of programmed reinforcers on the trained FI. Extinction performance, plotted cumulatively, showed marked individual variability, with higher extinction response rates generally observed following longer FI schedules (e.g., ~84 responses/min after FI 60 vs. ~10 after FI 20), and no clear relation to baseline (operant-level) rates. Observational notes suggested that self-generated behaviors (e.g., counting, self-talk, toy manipulation) appeared to modulate extinction, potentially functioning as discriminative or competing stimuli. The paper argues that, compared with nonhuman preparations, children introduce uncontrolled, response-produced stimuli that can either sustain or suppress responding during extinction, motivating future studies to bring such variables under experimental control."
    },
    {
        "title": "A Method for Obtaining Psychophysical Thresholds from the Pigeon",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "31-43",
        "authors": ["Blough, D. S."],
        "url": "https://doi.org/10.1901/jeab.1958.1-31",
        "process": ["Psychophysical Threshold", "Stimulus Control", "Shaping", "Schedule: Variable Ratio", "Schedule: Variable Interval", "Schedule: Fixed Ratio", "Reinforcement: Positive", "Punishment: Positive"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "A psychophysical method for measuring the pigeon’s absolute visual threshold, combining Bekesy’s human audiometric tracking technique with operant conditioning procedures is described. Pigeons peck one key (A) when a lighted stimulus patch is visible and another (B) when it appears dark. Pecking key A reduces stimulus intensity; pecking key B increases it. Correct sequences under variable-ratio and variable-interval schedules maintain stimulus control and drive the stimulus intensity to oscillate across the bird’s threshold, yielding a continuous graphical record. Special contingencies—such as penalties for incorrect pecks, enforced pauses when switching keys, occasional omission of light after reinforcement, and differentiated reinforcement for below-threshold vs. above-threshold conditions—enhance control and minimize artifacts like wandering thresholds. The procedure can be generalized to measure other sensory thresholds by substituting appropriate stimulus continua. Its applicability to both absolute and differential thresholds are discussed."
    },
    {
        "title": "The Behavioral Effects of Some Temporally Defined Schedules of Reinforcement",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "45-55",
        "authors": ["Hearst, E."],
        "url": "https://doi.org/10.1901/jeab.1958.1-45",
        "process": ["Schedule: Fixed Interval", "Schedule: Limited Hold", "Ratio-like Performance"],
        "static-equation": "",
        "static-equation-definitions": "", 
        "recursive-equation": "",
        "recursive-equation-definitions": "", 
        "abstract": "Pigeons were trained under reinforcement schedules defined by alternating periods during which a single response could be reinforced ($t_D$) with periods in which no reinforcement was available ($t_A$). The ratio $T = t_D/(t_D + t_A)$ was varied systematically while the total cycle length remained constant at 30 seconds. Four subjects were studied as $T$ was reduced from 1.00 (equivalent to a fixed-interval 30-second schedule) to values as low as 0.013. Decreases in $T$ produced marked increases in overall response rates and responses per reinforcement, and cumulative records shifted from patterns resembling fixed-interval performance to those characteristic of fixed-ratio or variable-ratio schedules. At the smallest values of $T$, some birds showed irregularities and pauses comparable to \\\"ratio strain.\\\" Interresponse-time distributions revealed a progressive increase in the relative frequency of short IRTs with decreasing $T$. The results demonstrate that interval-like and ratio-like performances can be obtained within a single framework of temporally defined variables, and that systematic changes in limited-hold duration generate a transition from interval to ratio behavior."
      },
    {
        "title": "The effects of deprivation upon temporally spaced responding",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "59-65",
        "authors": ["Conrad, D. G.", "Sidman, M.", "Herrnstein, R. J."],
        "url": "https://doi.org/10.1901/jeab.1958.1-59",
        "process": ["Timing", "DRL", "Stimulus Generalization"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "Five rats and a rhesus monkey were trained to space their responses at least 20 seconds apart. The performances of the monkey and one rat were tested after five periods of deprivation ranging from zero to 72 hours and 9 to 69. 5 hours, re-spectively. Two rats received a 10-hour test session during which the accumu-lation of reinforcements produced satiation. Two other rats were trained to spacetheir responses by at least 20 seconds but not more than 24 seconds. The last twoanimals were also tested in a 10-hour satiation session.In all cases the major effect upon performance of manipulating deprivation inthe DRL situation was observed to occur only after short deprivations or when theanimals were near satiation. With greater satiation, long IRT' s became more fre-quent and response rates dropped. Over a wide range of deprivations, little changein the performance could be detected."
    }, 
    {
        "title": "Diagramming Schedules of Reinforcement",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "67-68",
        "authors": ["Skinner, B. F."],
        "url": "https://doi.org/10.1901/jeab.1958.1-67",
        "process": ["Schedules of Reinforcement", "Schedule: Fixed Interval", "Schedule: Fixed Ratio", "Schedule: Variable Ratio", "Schedule: Variable Interval", "Schedule: Interlocking", "Schedule: Tandem", "Schedule: Chained", "Schedule: Mixed", "Schedule: Multiple", "Reinforcement"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "A graphical method for representing schedules of reinforcement is described. Cumulative records of responding are used as coordinates, with reinforcement specified by lines relating responses to time. The system depicts ratio and interval schedules, as well as their variable, alternative, conjunctive, interlocking, tandem, chained, mixed, and multiple forms. The method provides a compact notation for analyzing contingencies of reinforcement and suggests possible extensions to aversive control and punishment."
    }, 
    {
        "title": "Avoidance Behavior and the Development of Gastroduodenal Ulcers",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "69-72",
        "authors": ["Brady, J. V.", "Porter, R. W.", "Conrad, D. G.", "Mason, J. W."],
        "url": "https://doi.org/10.1901/jeab.1958.1-69",
        "process": ["Avoidance"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "Eight rhesus monkeys were studied in a yoked-chair avoidance procedure in which one animal could postpone footshock for both members of the pair by lever pressing. Experimental animals developed stable avoidance responding, while yoked controls received identical shock exposure without control over its occurrence. Over weeks of continuous testing, all avoidance animals developed severe gastroduodenal ulceration, while controls did not. Measures of corticosteroid excretion showed no consistent elevations. The findings suggest that the behavioral contingencies of avoidance, rather than physical shock exposure alone, may be critical in the etiology of gastrointestinal pathology under conditions of prolonged stress."
    }, 
    {
        "title": "Effects of Chlorpromazine and Promazine on Performance on a Mixed Schedule of Reinforcement",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "73-82",
        "authors": ["Dews, P. B."],
        "url": "https://doi.org/10.1901/jeab.1958.1-73",
        "process": ["Schedule: Mixed", "Schedule: Fixed Interval", "Schedule: Fixed Ratio", "Behavioral Pharmacology"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "Drugs are known to alter operant behavior, yet their influence on discriminations without explicit external cues was less defined. This study examined the effects of chlorpromazine and promazine on pigeons responding under a mixed FI 15/FR 50 schedule. Four birds were maintained at 80% body weight and tested across repeated daily sessions, with drug or control injections administered prior to performance. Under control conditions, stable responding developed, including second-order effects where FI pauses varied depending on preceding schedule components. Both drugs reduced the characteristic postreinforcement pauses in fixed-interval components, with promazine at higher doses producing striking increases in overall response output compared to chlorpromazine. Importantly, drug effects selectively disrupted subtle schedule-controlled discriminations while leaving gross discriminations and ratio performance largely intact. These findings underscore the utility of complex schedules as analytic tools for detecting differential drug effects on behavioral organization."
    },
    {
        "title": "The Grooming Behavior of the Chimpanzee as a Reinforcer",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "83-85",
        "authors": ["Falk, J. L."],
        "url": "https://doi.org/10.1901/jeab.1958.1-83",
        "process": ["Discrimination Learning", "Reinforcement: Positive"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "Grooming is a pervasive primate social behavior, yet its potential function as a reinforcer had not been directly tested. A young chimpanzee was provided access to grooming the experimenter’s arm contingent upon correct visual discrimination responses. Thirty-second periods of grooming, signaled by a clicker, reliably maintained responding, and the animal acquired both the original discrimination (square vs. cross) and a subsequent reversal. Despite its immature age, the subject showed strong motivation for grooming, likely amplified by extended social isolation. The study demonstrates that grooming, beyond its social and hygienic functions, can serve as an effective reinforcer for operant behavior. These findings extend the range of known reinforcers and suggest that species-typical social activities may be integrated into experimental analyses of behavior."
    },
    {
        "title": "Stimulus-Producing Responses in Chimpanzees",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "87-102",
        "authors": ["Kelleher, R. T."],
        "url": "https://doi.org/10.1901/jeab.1958.1-87",
        "process": ["Discrimination Learning", "Schedule: Fixed Interval", "Schedule: Variable Ratio", "Reinforcement: Positive", "Observing Responses"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "Discrimination learning often requires not only food-producing responses (Rf) but also stimulus-producing responses (Rs) that bring discriminative cues under experimental control. Previous analyses inferred Rs indirectly, but this study directly examined them in chimpanzees. Using a two-key procedure, one key delivered food under fixed-interval or variable-ratio schedules (Rf), while the other produced red or blue lights correlated with reinforcement or extinction (Rs). Across multiple experiments, chimpanzees maintained substantial Rs rates, especially under fixed-ratio reinforcement of stimuli, and discriminations were strong (DRs near 1.0). Extinction or nondifferential stimuli rapidly abolished Rs performance, confirming that discriminative function—not mere stimulus onset—maintains responding. The work establishes Rs as operant behaviors governed by reinforcement schedules and provides an objective method for studying “observing responses.” These findings highlight Rs as essential behavioral processes for analyzing discrimination, attention, and conditioned reinforcement."
    },
    {
        "title": "Some Factors Involved in the Stimulus Control of Operant Behavior",
        "year": 1958,
        "volume": 1,
        "issue": 1,
        "pages": "103-107",
        "authors": ["Morse, W. H.", "Skinner, B. F."],
        "url": "https://doi.org/10.1901/jeab.1958.1-103",
        "process": ["Stimulus Control", "Schedule: Variable Interval", "Extinction", "Reinforcement: Positive"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "Stimulus control in operant behavior arises when reinforcement delivered in the presence of a discriminative stimulus (SD) alters the subsequent probability of responding. Earlier research had shown that temporal pairing of SD and reinforcement could produce generalization gradients, but the role of the operant response itself remained uncertain. This study tested whether stimulus–reinforcer pairing, independent of the response, was sufficient to establish stimulus control. Pigeons first received food on a variable-interval schedule in the presence of one color light but never in another. Later, key pecking was conditioned in white light, then extinguished while the colored lights alternated. Birds responded preferentially in the stimulus previously correlated with food, though less strongly than if responses had been differentially reinforced. These results confirm that temporal pairing of stimulus and reinforcement alone contributes to stimulus control, though direct response–stimulus contingencies sharpen the effect. The findings clarify boundary conditions for how discriminative stimuli acquire control of operant behavior."
    },
    {
        "title": "Probability Relations within Response Sequences Under Ratio Reinforcement",
        "year": 1958,
        "volume": 1,
        "issue": 2,
        "pages": "109-121",
        "authors": ["Mechner, F."],
        "url": "https://doi.org/10.1901/jeab.1958.1-109",
        "process": ["Schedule: Fixed Ratio", "Reinforcement: Positive", "Discrimination Learning", "Response Produced Stimuli"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "A procedure was developed for the purpose of investigating the internal cohesion of response sequences maintained on fixed-ratio reinforcement. Under this procedure, rats were trained in a two-lever Skinner box on a schedule wherein water reinforcement was delivered either upon the completion of N consecutive responses on lever A, or else upon the completion of a minimum of N consecutive responses on lever A followed by an additional response on lever B. A random programmer determined which of these two conditions prevailed on any run. The two main parameters investigated were N (the minimum number of responses required for reinforcement) and P (the probability that the animal is reinforced immediately upon the completion of the N responses on lever A). The values of N used were 4, 8, 12, and 16, and the values of P were 0.00, 0.25, 0.50, and 0.75. The following two functions were calculated for the data of each animal for each of the four values of the variable to which it was exposed: (a) the probability of switching to lever B as a function of the number of responses already made on lever A, and (b) the frequency distributions of lengths of runs. The effect of increasing N was a shift in both of these functions upward along the abscissa, in such a way that the sharpest rise in function (a) and the median of distribution (b) always fell slightly above the stipulated value of N. The effect of increasing P was qualitatively similar to the effect of increasing N."
    },
    {
        "title": "Preference and switching under concurrent scheduling",
        "year": 1958,
        "volume": 1,
        "issue": 2,
        "pages": "123-144",
        "authors": ["Findley, J. D."],
        "url": "https://doi.org/10.1901/jeab.1958.1-123",
        "process": ["Schedule: Concurrent", "Reinforcement: Positive", "Stimulus Control", "Schedule: Variable Interval", "Schedule: Fixed Interval", "Schedule: Progressive Interval", "Schedule: Fixed Ratio", "Schedule: Chained"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "A series of experiments is reported in which pigeons were trained to peck an illuminated key for grain. The color of this key was either red or green, and associated with each color was a given reinforcement schedule. Pecking on a second key permitted the birds to switch the color appearing on the first key. The general behavior resulting from this type of procedure suggested an operant chain in which pecking on the second key was maintained by its consequences for reinforcement on the first key. Preferences for a give color and rate of switching colors were found to be a function of the particular schedules and switching contingencies imposed."
    },
    {
        "title": "Shock intensity in variable-interval escape schedules",
        "year": 1958,
        "volume": 1,
        "issue": 2,
        "pages": "145-148",
        "authors": ["Dinsmoor, J. A., & Winograd, E."],
        "url": "https://doi.org/10.1901%2Fjeab.1958.1-145",
        "process": ["Schedule: Variable Interval", "Reinforcement: Negative", "Escape"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "When the intensity of the shock is raised or lowered for rats pressing a bar on a variable-interval escape procedure, the transition to a new rate is usually immediate. The rates are roughly proportional to the level of current but show more moderate positive acceleration from 40 to at least 400 microamperes."
    }, 
    {
        "title": "A source of error in estimating the number of reinforcements in a lever pressing apparatus",
        "year": 1958,
        "volume": 1,
        "issue": 2,
        "pages": "149-152",
        "authors": ["Hurwitz, H. M. B."],
        "url": "https://doi.org/10.1901/jeab.1958.1-149",
        "process": ["Reinforcement: Positive", "Response Bouts", "Response Bursts", "Extinction"],
        "static-equation": "p(L|T) = n(L ∧ T) / n(T)",
        "static-equation-definitions": "p(L|T): Probability of a lever response given a trough response; n(L ∧ T): Number of times a lever response directly followed a trough response within the observation window; n(T): Total number of trough responses within the observation window.",
        "recursive-equation": "p_t(L|T) = p_{t-1}(L|T) + (1 / n_t(T)) * ( I_t(L|T) - p_{t-1}(L|T) )",
        "recursive-equation-definitions": "p_t(L|T): Estimated probability after t events in which a trough response could be followed by a lever response; p_{t-1}(L|T): Previous estimate; n_t(T): Cumulative count of trough responses up to event t; I_t(L|T): Indicator taking value 1 if event t is a lever response that directly follows a trough response, else 0. Initialize p_0(L|T) in [0,1] (e.g., 0.5). Update only on trials where a new T occurs; otherwise, retain the previous estimate.",
        "abstract": "Five male hooded rats were trained in a lever-pressing apparatus, food serving as reinforcement. Following two tourgh-training sessions, each subject was given three lever-pressing training sessions, a total of 150 reinforcements being given. A continuous record of lever and trough responses was obtained and subsequently analyzed. This showed that the probability of a lever response following a trough responses, i.e., p(L/T), as calculated over the final 40 reinforcements, ranges between 0.69 and 0.81. Furthermore, the probability is high that between the 50th and 110th lever response, subject engages in a response burst on the lever. This response bursts accounts for a considerable proportion of the difference between the total number of lever responses reinforced and the number of reinforcements obtained."
    }, 
    {
        "title": "Some effects of temporal variables on conditioned suppression",
        "year": 1958,
        "volume": 1,
        "issue": 2,
        "pages": "153-162",
        "authors": ["Stein, L., Sidman, M., & Brady, J. V."],
        "url": "https://doi.org/10.1901/jeab.1958.1-153",
        "process": ["Schedule: Fixed Interval", "Reinforcement: Negative", "Conditioned Suppression", "Shedule: Multiple"],
        "static-equation": "RD = \\log\\!\\left(\\tfrac{X}{Y}\\right) \\\\[6pt] L \\;=\\; \\mathbb{E}_{\\ell}\\!\\left[\\max\\!\\big(X - \\tfrac{1}{2}\\ell,\\,0\\big)\\right] \\;\\approx\\; \\sum_i d_i\\,\\max\\!\\big(X - \\tfrac{1}{2}\\ell_i,\\,0\\big) \\\\[4pt] \\text{(single VI interval } I \\text{ with } I\\ge X):\\quad L = \\tfrac{X^{2}}{2I} \\\\[6pt] M = \\tfrac{L}{\\bar{I}} \\\\[6pt] \\%\\,\\text{lost} = 100\\times \\tfrac{L}{X+Y}",
        "static-equation-definitions": "RD: Relative Duration predictor of suppression; X: stimulus-on duration in each cycle; Y: stimulus-off duration in each cycle; L: expected 'lock-up' time within one stimulus period X (time during which reinforcers are set up but cannot be obtained under complete suppression); d_i: relative weight for VI interval i (duration_i / sum of all durations); \\ell_i: duration of VI interval i; I: a specific VI interval length; \\bar{I}: mean inter-reinforcement interval of the VI schedule; M: expected number of missed reinforcers per stimulus period; % lost: percentage of total available reinforcers lost per X:Y cycle, equal to 100·L/(X+Y).",
        "recursive-equation": "g(\\ell) := \\max\\!\\big(X - \\tfrac{1}{2}\\ell,\\,0\\big) \\\\[4pt] L_t = L_{t-1} + \\tfrac{1}{n_t}\\big( g(\\ell_t) - L_{t-1} \\big) \\\\[4pt] \\bar{I}_t = \\bar{I}_{t-1} + \\tfrac{1}{n_t}\\big( \\Delta I_t - \\bar{I}_{t-1} \\big) \\\\[4pt] M_t = \\tfrac{L_t}{\\bar{I}_t} \\\\[4pt] \\%\\,\\text{lost}_t = 100\\times \\tfrac{L_t}{X+Y}",
        "recursive-equation-definitions": "g(\\ell): per-interval lock-up contribution function; L_t: running estimate of expected lock-up time within X after t observed VI intervals; n_t: number of VI intervals observed up to t (or sessions/blocks—use the same unit consistently); \\ell_t: duration of the VI interval in force at the onset of the t-th stimulus period; \\bar{I}_t: running estimate of the mean inter-reinforcement interval; \\Delta I_t: observed inter-reinforcement interval at step t; M_t: running estimate of expected missed reinforcers per stimulus period; % lost_t: running estimate of percent reinforcers lost per X:Y cycle. Use initialization L_0=0, \\bar{I}_0>0 (e.g., the programmed mean), and update once per stimulus period. For faster adaptation, replace 1/n_t with a constant learning rate \\alpha\\in(0,1).",
        "abstract": "The present experiment investigated two temporal variables in the Estes-Skinner conditioned suppression situation. The five animals were given prolonged training on a series of conditioned-suppression programs; the programs differed from each other with respect to the stimulus and between-stimulus durations. The degree of suppression varied widely from program to program. Roughly, programs in which the stimulus duration was short relative to the duration of the stimulus-off interval produced good suppression, and programs in which the stimulus duration was relatively long produced poor supression. These effects were largely reversible. \"Relative Duration\" defined as log(stimulus duration/stimulus-off duration), correlated 0.90 with the degree of behavioral suppression. The number of reinforcements obtained in each program was relatively constant at approximately 90% of the maximum number obtainable in an experimental session in spite of the wide differences in suppression. An estimate of the percentage of reinforcements that would be lost if the animal suppressed completely in the stimulus period was calculated for each conditioned-suppression program. This measure was correlated 0.92 with suppression scores, indicating that the strength of suppression in any program decreases to the extent that such suppression reduces opportunities for positive reinforcement."
      }, 
      {
        "title": "Intermittent reinforcement of a complex response in a chimpanzee",
        "year": 1958,
        "volume": 1,
        "issue": 2,
        "pages": "163-165",
        "authors": ["Ferster, C. B."],
        "url": "https://doi.org/10.1901/jeab.1958.1-163",
        "process": ["Schedule: Fixed Ratio", "Reinforcement: Positive"],
        "static-equation": "",
        "static-equation-definitions": "",
        "recursive-equation": "",
        "recursive-equation-definitions": "",
        "abstract": "The intermittently reinforced sequence was maintained with approximately the same number of \"errors\" as under continuous reinforcement. However, the percentage of sequences that did not conform to the reinforcement contingency decreased from 20% under continuous reinforcement to approximately 3% when 960 smissions of the reinforced sequence was required for an experimental session of 60 reinforcements, and 2% for the 1900 sequences required for an experimental sessions at a ratio of 33 to 1."
      }
]